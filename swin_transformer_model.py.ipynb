{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Standard library imports\n",
    "import os\n",
    "import random\n",
    "import gc\n",
    "import configparser\n",
    "import pickle\n",
    "import copy\n",
    "import tarfile\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from transformers import SwinForImageClassification, AutoImageProcessor, AutoModel, AutoModelForImageClassification\n",
    "from transformers import AdamW\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "\n",
    "# from peft import LoraConfig, get_peft_model, TaskType\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "image_folder = r\"C:\\Users\\Downloads\\train_images\"  # Folder containing images\n",
    "csv_path = r\"C:\\Users\\Downloads\\train.csv\"  # CSV file path\n",
    "output_csv = r\"C:\\Users\\Downloads\\matched_train.csv\"  # Output CSV file\n",
    "\n",
    "# Load CSV file\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Ensure image_id column matches extracted filenames\n",
    "df[\"image_id\"] = df[\"image_id\"].astype(str)  # Convert to string for matching\n",
    "\n",
    "# List available image files (only filenames without extension for matching)\n",
    "available_images = {os.path.splitext(f)[0] for f in os.listdir(image_folder)}\n",
    "\n",
    "# Filter DataFrame to only include images that exist in the folder\n",
    "matched_df = df[df[\"image_id\"].isin(available_images)]\n",
    "\n",
    "# Save matched results\n",
    "matched_df.to_csv(output_csv, index=False)\n",
    "\n",
    "print(f\"Matched dataset saved to {output_csv}\")\n",
    "print(f\"Total Matched Images: {len(matched_df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = r\"C:\\Users\\Downloads\\train.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Total rows in train.csv: {len(df)}\")\n",
    "print(f\"Unique image IDs in train.csv: {df['image_id'].nunique()}\")\n",
    "import os\n",
    "\n",
    "image_folder = r\"C:\\Users\\Downloads\\train_images\"\n",
    "image_files = os.listdir(image_folder)\n",
    "\n",
    "print(f\"Total images in folder: {len(image_files)}\")\n",
    "df[\"image_id\"] = df[\"image_id\"].astype(str) + \".png\"  # Ensure .png extension\n",
    "csv_images = set(df[\"image_id\"])\n",
    "folder_images = set(os.listdir(image_folder))\n",
    "\n",
    "# Find missing images\n",
    "missing_from_folder = csv_images - folder_images\n",
    "missing_from_csv = folder_images - csv_images\n",
    "\n",
    "print(f\"Images in CSV but not in folder: {len(missing_from_folder)}\")\n",
    "print(f\"Images in folder but not in CSV: {len(missing_from_csv)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = r\"C:\\Users\\Downloads\\train.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# Count duplicate image IDs\n",
    "duplicate_counts = df[\"image_id\"].value_counts()\n",
    "duplicates = duplicate_counts[duplicate_counts > 1]\n",
    "\n",
    "print(f\"Total duplicate entries: {duplicates.sum()}\")\n",
    "print(f\"Number of unique images with duplicates: {len(duplicates)}\")\n",
    "df = df.drop_duplicates(subset=\"image_id\")  # Keep only one entry per image\n",
    "clean_csv_path = r\"C:\\Users\\Downloads\\cleaned_train.csv\"\n",
    "df.to_csv(clean_csv_path, index=False)\n",
    "print(f\"Cleaned CSV saved to {clean_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV Columns: ['image_id', 'class_name', 'class_id', 'rad_id', 'x_min', 'y_min', 'x_max', 'y_max']\n",
      "First few rows:\n",
      "                            image_id          class_name  class_id rad_id  \\\n",
      "0  50a418190bc3fb1ef1633bf9678929b3          No finding        14    R11   \n",
      "1  21a10246a5ec7af151081d0cd6d65dc9          No finding        14     R7   \n",
      "2  9a5094b2563a1ef3ff50dc5c7ff71345        Cardiomegaly         3    R10   \n",
      "3  051132a778e61a86eb147c7c6f564dfe  Aortic enlargement         0    R10   \n",
      "4  063319de25ce7edb9b1c6b8881290140          No finding        14    R10   \n",
      "\n",
      "    x_min   y_min   x_max   y_max  \n",
      "0     NaN     NaN     NaN     NaN  \n",
      "1     NaN     NaN     NaN     NaN  \n",
      "2   691.0  1375.0  1653.0  1831.0  \n",
      "3  1264.0   743.0  1611.0  1019.0  \n",
      "4     NaN     NaN     NaN     NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "csv_path = r\"C:\\Users\\Downloads\\cleaned_train.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(\"CSV Columns:\", df.columns.tolist())  # Print all column names\n",
    "print(\"First few rows:\\n\", df.head())  # Show first few rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "IMAGE_FOLDER = r\"C:\\Users\\Downloads\\train_images\"  # Image folder\n",
    "LABEL_CSV = r\"C:\\Users\\Downloads\\cleaned_train.csv\"  # Cleaned CSV\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(LABEL_CSV)\n",
    "\n",
    "# Use 'class_id' as the label for stratification\n",
    "label_column = \"class_id\"\n",
    "if label_column not in df.columns:\n",
    "    raise ValueError(f\"❌ Column '{label_column}' not found in CSV! Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"✅ Using '{label_column}' as label column\")\n",
    "\n",
    "# Drop any NaN labels\n",
    "df = df.dropna(subset=[label_column])\n",
    "\n",
    "# Split into Train (80%), Test (10%), Validation (10%)\n",
    "train_df, temp_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[label_column])\n",
    "test_df, val_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df[label_column])\n",
    "\n",
    "# Save split datasets\n",
    "train_csv_path = r\"C:\\Users\\Downloads\\train_labels.csv\"\n",
    "test_csv_path = r\"C:\\Users\\Downloads\\test_labels.csv\"\n",
    "val_csv_path = r\"C:\\Users\\Downloads\\val_labels.csv\"\n",
    "\n",
    "train_df.to_csv(train_csv_path, index=False)\n",
    "test_df.to_csv(test_csv_path, index=False)\n",
    "val_df.to_csv(val_csv_path, index=False)\n",
    "\n",
    "print(f\"✅ Train labels saved to {train_csv_path} ({len(train_df)} samples)\")\n",
    "print(f\"✅ Test labels saved to {test_csv_path} ({len(test_df)} samples)\")\n",
    "print(f\"✅ Validation labels saved to {val_csv_path} ({len(val_df)} samples)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Train-Val Leakage: False\n",
      "🔍 Train-Test Leakage: False\n",
      "🔍 Val-Test Leakage: False\n",
      "✅ Data Cleaning Completed!\n"
     ]
    }
   ],
   "source": [
    "# Load the newly split datasets\n",
    "train_labels_df = pd.read_csv(train_csv_path)\n",
    "test_labels_df = pd.read_csv(test_csv_path)\n",
    "val_labels_df = pd.read_csv(val_csv_path)\n",
    "\n",
    "# Function to check for data leakage\n",
    "def check_for_leakage(df1, df2, column=\"image_id\"):\n",
    "    \"\"\"\n",
    "    Check if there is any data leakage between two datasets based on a unique identifier.\n",
    "    \"\"\"\n",
    "    df1_unique = set(df1[column].values)\n",
    "    df2_unique = set(df2[column].values)\n",
    "    return len(df1_unique.intersection(df2_unique)) > 0\n",
    "\n",
    "# Check for data leakage between splits\n",
    "leakage_train_val = check_for_leakage(train_labels_df, val_labels_df, \"image_id\")\n",
    "leakage_train_test = check_for_leakage(train_labels_df, test_labels_df, \"image_id\")\n",
    "leakage_val_test = check_for_leakage(val_labels_df, test_labels_df, \"image_id\")\n",
    "\n",
    "print(f\"🔍 Train-Val Leakage: {leakage_train_val}\")\n",
    "print(f\"🔍 Train-Test Leakage: {leakage_train_test}\")\n",
    "print(f\"🔍 Val-Test Leakage: {leakage_val_test}\")\n",
    "\n",
    "# Ensure bounding box values are numeric (convert and fill NaNs)\n",
    "bbox_cols = [\"x_min\", \"y_min\", \"x_max\", \"y_max\"]\n",
    "for df in [train_labels_df, test_labels_df, val_labels_df]:\n",
    "    df[bbox_cols] = df[bbox_cols].apply(pd.to_numeric, errors='coerce')\n",
    "    df[bbox_cols] = df[bbox_cols].fillna(0)  # Replace NaNs with 0\n",
    "\n",
    "print(\"✅ Data Cleaning Completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Train-Val Leakage: False\n",
      "🔍 Train-Test Leakage: False\n",
      "🔍 Val-Test Leakage: False\n"
     ]
    }
   ],
   "source": [
    "# Function to check for data leakage\n",
    "def check_for_leakage(df1, df2, column=\"image_id\"):\n",
    "    \"\"\"\n",
    "    Checks if there is any data leakage between two datasets based on a unique identifier (e.g., image_id).\n",
    "    Returns True if there is leakage, otherwise False.\n",
    "    \"\"\"\n",
    "    df1_unique = set(df1[column].values)\n",
    "    df2_unique = set(df2[column].values)\n",
    "    \n",
    "    leaked_samples = df1_unique.intersection(df2_unique)\n",
    "    \n",
    "    if leaked_samples:\n",
    "        print(f\"⚠️ WARNING: {len(leaked_samples)} leaked samples found!\")\n",
    "    \n",
    "    return len(leaked_samples) > 0\n",
    "\n",
    "# Check for data leakage between Train, Validation, and Test sets\n",
    "leakage_train_val = check_for_leakage(train_labels_df, val_labels_df, \"image_id\")\n",
    "leakage_train_test = check_for_leakage(train_labels_df, test_labels_df, \"image_id\")\n",
    "leakage_val_test = check_for_leakage(val_labels_df, test_labels_df, \"image_id\")\n",
    "\n",
    "# Print results\n",
    "print(f\"🔍 Train-Val Leakage: {leakage_train_val}\")\n",
    "print(f\"🔍 Train-Test Leakage: {leakage_train_test}\")\n",
    "print(f\"🔍 Val-Test Leakage: {leakage_val_test}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Leakage**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "class MedDataset(Dataset):\n",
    "    def __init__(self, data_dir, labels_df, image_processor, transforms=None):\n",
    "        \"\"\"\n",
    "        Custom Dataset class for medical images.\n",
    "\n",
    "        Args:\n",
    "            data_dir (str): Path to the directory containing images.\n",
    "            labels_df (pd.DataFrame): DataFrame containing image file names and corresponding labels.\n",
    "            image_processor (AutoImageProcessor): Transformer image processor for preprocessing.\n",
    "            transforms (callable, optional): Optional transformations to apply on images.\n",
    "        \"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.labels_df = labels_df\n",
    "        self.image_processor = image_processor\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels_df)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # Get the image name and corresponding labels\n",
    "        image_name = self.labels_df.iloc[index][\"image_id\"]  # Adjusted to match your column name\n",
    "        labels = self.labels_df.iloc[index, 1:].values.astype(float)  # Exclude 'image_id' column\n",
    "        \n",
    "        # Construct the image path\n",
    "        image_path = os.path.join(self.data_dir, image_name)\n",
    "        \n",
    "        # Open the image and convert to RGB\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        \n",
    "        # Apply preprocessing using the transformer image processor\n",
    "        processed_image = self.image_processor(image, return_tensors=\"pt\")\n",
    "        image_tensor = processed_image[\"pixel_values\"].squeeze(0)\n",
    "\n",
    "        # Apply additional transformations if specified\n",
    "        if self.transforms:\n",
    "            image_tensor = self.transforms(image_tensor)\n",
    "        \n",
    "        labels_tensor = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        return image_tensor, labels_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_name = \"microsoft/swin-tiny-patch4-window7-224\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "swin_image_processor = AutoImageProcessor.from_pretrained(checkpoint_name)\n",
    "swin_model = AutoModelForImageClassification.from_pretrained(checkpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ DataLoaders initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define dataset directories (update these with actual paths)\n",
    "TRAIN_DATASET = r\"C:\\Users\\Downloads\\train_images\"\n",
    "VAL_DATASET = r\"C:\\Users\\Downloads\\train_images\"  # Assuming the same folder for validation\n",
    "TEST_DATASET = r\"C:\\Users\\Downloads\\train_images\"  # Assuming the same folder for test\n",
    "\n",
    "# Load labels from your saved CSV files\n",
    "train_labels_raw_df = pd.read_csv(r\"C:\\Users\\Downloads\\train_labels.csv\")\n",
    "val_labels_raw_df = pd.read_csv(r\"C:\\Users\\Downloads\\val_labels.csv\")\n",
    "test_labels_raw_df = pd.read_csv(r\"C:\\Users\\\\Downloads\\test_labels.csv\")\n",
    "\n",
    "# Initialize datasets\n",
    "train_dataset = MedDataset(\n",
    "    data_dir=TRAIN_DATASET,\n",
    "    labels_df=train_labels_raw_df,\n",
    "    image_processor=swin_image_processor\n",
    ")\n",
    "\n",
    "val_dataset = MedDataset(\n",
    "    data_dir=VAL_DATASET,\n",
    "    labels_df=val_labels_raw_df,\n",
    "    image_processor=swin_image_processor\n",
    ")\n",
    "\n",
    "test_dataset = MedDataset(\n",
    "    data_dir=TEST_DATASET,\n",
    "    labels_df=test_labels_raw_df,\n",
    "    image_processor=swin_image_processor\n",
    ")\n",
    "\n",
    "# Define data loader parameters\n",
    "batch_size = 128\n",
    "num_workers = 4\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "print(\"✅ DataLoaders initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.485,\n",
       "    0.456,\n",
       "    0.406\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.229,\n",
       "    0.224,\n",
       "    0.225\n",
       "  ],\n",
       "  \"resample\": 3,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_processor = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = swin_model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gelu'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_config.hidden_act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinForImageClassification(\n",
       "  (swin): SwinModel(\n",
       "    (embeddings): SwinEmbeddings(\n",
       "      (patch_embeddings): SwinPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): SwinEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "            (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "            (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-5): 6 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (downsample): SwinPatchMerging(\n",
       "            (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "            (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): SwinStage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x SwinLayer(\n",
       "              (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (attention): SwinAttention(\n",
       "                (self): SwinSelfAttention(\n",
       "                  (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): SwinSelfOutput(\n",
       "                  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (drop_path): SwinDropPath(p=0.1)\n",
       "              (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (intermediate): SwinIntermediate(\n",
       "                (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (intermediate_act_fn): GELUActivation()\n",
       "              )\n",
       "              (output): SwinOutput(\n",
       "                (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=768, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModuleList(\n",
       "  (0): SwinStage(\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x SwinLayer(\n",
       "        (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SwinAttention(\n",
       "          (self): SwinSelfAttention(\n",
       "            (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): SwinSelfOutput(\n",
       "            (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): SwinDropPath(p=0.1)\n",
       "        (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "        (intermediate): SwinIntermediate(\n",
       "          (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): SwinOutput(\n",
       "          (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (downsample): SwinPatchMerging(\n",
       "      (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "      (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (1): SwinStage(\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x SwinLayer(\n",
       "        (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SwinAttention(\n",
       "          (self): SwinSelfAttention(\n",
       "            (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): SwinSelfOutput(\n",
       "            (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): SwinDropPath(p=0.1)\n",
       "        (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "        (intermediate): SwinIntermediate(\n",
       "          (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): SwinOutput(\n",
       "          (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (downsample): SwinPatchMerging(\n",
       "      (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (2): SwinStage(\n",
       "    (blocks): ModuleList(\n",
       "      (0-5): 6 x SwinLayer(\n",
       "        (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SwinAttention(\n",
       "          (self): SwinSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): SwinSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): SwinDropPath(p=0.1)\n",
       "        (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "        (intermediate): SwinIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): SwinOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (downsample): SwinPatchMerging(\n",
       "      (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "      (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (3): SwinStage(\n",
       "    (blocks): ModuleList(\n",
       "      (0-1): 2 x SwinLayer(\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): SwinAttention(\n",
       "          (self): SwinSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): SwinSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (drop_path): SwinDropPath(p=0.1)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (intermediate): SwinIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): SwinOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_model.swin.encoder.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(swin_model.swin.encoder.layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stage 0: The first stage has 2 blocks with 96 features. This stage is responsible for processing lower-level information such as edges and textures.\n",
    "* Stage 1: The second stage has 2 blocks with 192 features. This stage processes more complex features, combining the initial low-level information.\n",
    "* Stage 2: The third stage has 6 blocks with 384 features. At this point, the model captures more semantic information and high-level patterns.\n",
    "* Stage 3: The fourth stage has 2 blocks with 768 features. This stage is responsible for the most abstract features and task-specific learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Hidden Size: 768\n",
      "Activation Function: gelu\n",
      "Classifier: Linear(in_features=768, out_features=1000, bias=True)\n",
      "Encoder Layers: 4\n",
      "✅ Model initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForImageClassification\n",
    "\n",
    "# Configuration Variables\n",
    "PRETRAINED_MODEL_NAME = \"microsoft/swin-tiny-patch4-window7-224\"\n",
    "NUM_CLASSES = 14\n",
    "HIDDEN_DIM = 512\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.1\n",
    "UNFREEZE_LAYERS = 2  # Number of layers to unfreeze\n",
    "\n",
    "# Model Configuration\n",
    "swin_model = AutoModelForImageClassification.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model_config = swin_model.config\n",
    "hidden_size = model_config.hidden_size\n",
    "hidden_act = model_config.hidden_act\n",
    "\n",
    "print(f\"Model Hidden Size: {hidden_size}\")\n",
    "print(f\"Activation Function: {hidden_act}\")\n",
    "\n",
    "# Swin Model Details\n",
    "print(f\"Classifier: {swin_model.classifier}\")\n",
    "print(f\"Encoder Layers: {len(swin_model.swin.encoder.layers)}\")\n",
    "\n",
    "\n",
    "class SwinModelWithPEFT(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer Model with optional Parameter-Efficient Fine-Tuning (PEFT).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name=PRETRAINED_MODEL_NAME,\n",
    "        lora_r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        unfreeze_layers=UNFREEZE_LAYERS,\n",
    "    ):\n",
    "        super(SwinModelWithPEFT, self).__init__()\n",
    "\n",
    "        # Load Pretrained Swin Model\n",
    "        self.backbone = AutoModelForImageClassification.from_pretrained(pretrained_model_name)\n",
    "        self.backbone.classifier = nn.Identity()  # Remove classifier for feature extraction\n",
    "\n",
    "        # LoRA - Skipped due to time constraints, but left for reference\n",
    "        \"\"\"\n",
    "        lora_config = LoraConfig(\n",
    "            r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "            target_modules=[\"query\", \"value\"],\n",
    "            task_type=\"FEATURE_EXTRACTION\"\n",
    "        )\n",
    "        self.backbone = get_peft_model(self.backbone, lora_config)\n",
    "        \"\"\"\n",
    "\n",
    "        # Unfreeze selected layers\n",
    "        self._unfreeze_layers(unfreeze_layers)\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.backbone(pixel_values=pixel_values)\n",
    "        return outputs\n",
    "\n",
    "    def _unfreeze_layers(self, num_layers_to_unfreeze):\n",
    "        \"\"\"\n",
    "        Unfreeze the last `num_layers_to_unfreeze` layers of the Swin model.\n",
    "        \"\"\"\n",
    "        stages = self.backbone.swin.encoder.layers\n",
    "        total_stages = len(stages)\n",
    "\n",
    "        for i, stage in enumerate(stages):\n",
    "            if i < total_stages - num_layers_to_unfreeze:\n",
    "                for param in stage.parameters():\n",
    "                    param.requires_grad = False\n",
    "            else:\n",
    "                for block in stage.blocks:\n",
    "                    for param in block.parameters():\n",
    "                        param.requires_grad = True\n",
    "\n",
    "    def freeze_all_layers(self):\n",
    "        \"\"\"Freeze all layers of the backbone model.\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def unfreeze_all_layers(self):\n",
    "        \"\"\"Unfreeze all layers of the backbone model.\"\"\"\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "\n",
    "class MultiLabelClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Fully connected classifier for multi-label classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim, hidden_dim=HIDDEN_DIM, num_classes=NUM_CLASSES):\n",
    "        super(MultiLabelClassifier, self).__init__()\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(feature_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, num_classes),\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, features):\n",
    "        return self.classifier(features)\n",
    "\n",
    "\n",
    "class MultiLabelModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full model combining feature extraction and multi-label classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        pretrained_model_name=PRETRAINED_MODEL_NAME,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        hidden_dim=HIDDEN_DIM,\n",
    "        lora_r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "    ):\n",
    "        super(MultiLabelModel, self).__init__()\n",
    "\n",
    "        # Feature Extractor (Swin Model with optional PEFT)\n",
    "        self.feature_extractor = SwinModelWithPEFT(\n",
    "            pretrained_model_name=pretrained_model_name,\n",
    "            lora_r=lora_r,\n",
    "            lora_alpha=lora_alpha,\n",
    "            lora_dropout=lora_dropout,\n",
    "        )\n",
    "\n",
    "        feature_dim = self.feature_extractor.backbone.config.hidden_size\n",
    "\n",
    "        # Classifier Head\n",
    "        self.classifier = MultiLabelClassifier(\n",
    "            feature_dim=feature_dim,\n",
    "            hidden_dim=hidden_dim,\n",
    "            num_classes=num_classes,\n",
    "        )\n",
    "\n",
    "    def forward(self, pixel_values):\n",
    "        outputs = self.feature_extractor(pixel_values=pixel_values)\n",
    "        features = outputs.logits  # Extract features\n",
    "        logits = self.classifier(features)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = MultiLabelModel()\n",
    "print(\"✅ Model initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Display column names\n",
    "print(\"Columns in dataset:\", train_labels_raw_df.columns.tolist())\n",
    "\n",
    "# Print distribution for each label\n",
    "for column in train_labels_raw_df.columns[1:]:\n",
    "    print(f\"\\nDistribution of {column}:\")\n",
    "    print(train_labels_raw_df[column].value_counts())\n",
    "\n",
    "# Plot distributions\n",
    "plt.figure(figsize=(12, 8))\n",
    "for i, column in enumerate(train_labels_raw_df.columns[1:]):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    sns.countplot(x=column, data=train_labels_raw_df, palette=\"coolwarm\")\n",
    "    plt.title(f\"Distribution of {column}\")\n",
    "    plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all label columns are numeric\n",
    "train_labels_raw_df.iloc[:, 1:] = train_labels_raw_df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Total number of samples\n",
    "total_samples = len(train_labels_raw_df)\n",
    "\n",
    "# Count number of positive samples per class\n",
    "positive_counts = train_labels_raw_df.iloc[:, 1:].sum(axis=0)\n",
    "\n",
    "# Avoid division by zero: Replace zero counts with a small value\n",
    "positive_counts = positive_counts.replace(0, 1e-6)\n",
    "\n",
    "# Compute class weights using inverse frequency method\n",
    "class_weights = total_samples / positive_counts\n",
    "\n",
    "# Normalize weights (divide by max weight to keep scale reasonable)\n",
    "normalized_class_weights = class_weights / class_weights.max()\n",
    "\n",
    "# Print computed class weights\n",
    "print(\"\\nComputed Class Weights:\\n\", normalized_class_weights)\n",
    "\n",
    "# Verify normalization\n",
    "sum_normalized = normalized_class_weights.sum()\n",
    "sum_original = class_weights.sum()\n",
    "print(f\"Normalized Sum: {sum_normalized}, Original Sum: {sum_original}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive Counts per Class:\\n\", positive_counts)\n",
    "positive_counts = positive_counts.replace(0, 1e-6)\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure all label columns are numeric\n",
    "train_labels_raw_df.iloc[:, 1:] = train_labels_raw_df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Total number of samples\n",
    "total_samples = len(train_labels_raw_df)\n",
    "\n",
    "# Count number of positive samples per class\n",
    "positive_counts = train_labels_raw_df.iloc[:, 1:].sum(axis=0)\n",
    "\n",
    "# Debugging: Print positive_counts to check for zeros\n",
    "print(\"Positive Counts per Class:\\n\", positive_counts)\n",
    "\n",
    "# Avoid division by zero: Replace zero counts with a small value\n",
    "positive_counts = positive_counts.replace(0, 1e-6)\n",
    "\n",
    "# Compute class weights using inverse frequency method\n",
    "class_weights = total_samples / positive_counts\n",
    "\n",
    "# Normalize weights (divide by max weight to keep scale reasonable)\n",
    "normalized_class_weights = class_weights / class_weights.max()\n",
    "\n",
    "# Print computed class weights\n",
    "print(\"\\nComputed Class Weights:\\n\", normalized_class_weights)\n",
    "\n",
    "# Verify normalization\n",
    "sum_normalized = normalized_class_weights.sum()\n",
    "sum_original = class_weights.sum()\n",
    "print(f\"Normalized Sum: {sum_normalized}, Original Sum: {sum_original}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert all columns except the first (ID column) to numeric\n",
    "train_labels_raw_df.iloc[:, 1:] = train_labels_raw_df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Select only class label columns (excluding 'class_name', 'rad_id', and any coordinate columns)\n",
    "class_label_columns = [col for col in train_labels_raw_df.columns if 'class' in col.lower()]\n",
    "train_labels_cleaned = train_labels_raw_df[class_label_columns]\n",
    "\n",
    "# Total number of samples\n",
    "total_samples = len(train_labels_cleaned)\n",
    "\n",
    "# Count number of positive samples per class\n",
    "positive_counts = train_labels_cleaned.sum(axis=0)\n",
    "\n",
    "# Debugging: Print positive_counts to check for zeros\n",
    "print(\"Filtered Positive Counts per Class:\\n\", positive_counts)\n",
    "\n",
    "# Avoid division by zero: Replace zero counts with a small value\n",
    "positive_counts = positive_counts.replace(0, 1e-6)\n",
    "\n",
    "# Compute class weights using inverse frequency method\n",
    "class_weights = total_samples / positive_counts\n",
    "\n",
    "# Normalize weights (divide by max weight to keep scale reasonable)\n",
    "normalized_class_weights = class_weights / class_weights.max()\n",
    "\n",
    "# Print computed class weights\n",
    "print(\"\\nComputed Class Weights:\\n\", normalized_class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for visualization\n",
    "class_counts = train_labels_raw_df.iloc[:, 1:].sum()\n",
    "class_dist_df = pd.DataFrame({'Class': class_counts.index, 'Count': class_counts.values})\n",
    "class_dist_df = class_dist_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Print class distribution\n",
    "print(\"\\nOverall Sample Distribution:\")\n",
    "print(class_dist_df)\n",
    "\n",
    "# Plot class distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(x='Class', y='Count', data=class_dist_df, palette=\"viridis\")\n",
    "plt.title(\"Overall Sample Distribution of Classes\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Class\", fontsize=12)\n",
    "plt.ylabel(\"Number of Positive Samples\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ModifiedWeightedFocalLoss(nn.Module):\n",
    "    def __init__(self, class_weights=None, gamma=2.0, alpha=0.25, reduction=\"mean\"):\n",
    "        \"\"\"\n",
    "        Implements Focal Loss with optional class weighting.\n",
    "        - gamma: Focal Loss focusing parameter\n",
    "        - alpha: Balances positive vs negative samples\n",
    "        - class_weights: Precomputed class-wise weights for handling imbalance\n",
    "        \"\"\"\n",
    "        super(ModifiedWeightedFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "\n",
    "        # Convert class weights to tensor if provided\n",
    "        if class_weights is not None:\n",
    "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "        else:\n",
    "            self.class_weights = torch.ones(14)  # Default to equal weighting\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        \"\"\"\n",
    "        Computes the focal loss.\n",
    "        - inputs: Model predictions (logits)\n",
    "        - targets: True labels (binary)\n",
    "        \"\"\"\n",
    "        # Apply sigmoid activation\n",
    "        probas = torch.sigmoid(inputs)\n",
    "\n",
    "        # Compute Binary Cross Entropy (BCE) loss\n",
    "        bce_loss = F.binary_cross_entropy(probas, targets, reduction=\"none\")\n",
    "\n",
    "        # Compute modulating factor: (1 - p_t)^gamma\n",
    "        p_t = probas * targets + (1 - probas) * (1 - targets)\n",
    "        modulating_factor = (1 - p_t) ** self.gamma\n",
    "\n",
    "        # Compute final weighted focal loss\n",
    "        focal_loss = self.alpha * modulating_factor * bce_loss\n",
    "\n",
    "        # Apply class weights\n",
    "        weighted_loss = focal_loss * self.class_weights.to(inputs.device)\n",
    "\n",
    "        # Compute final loss reduction\n",
    "        if self.reduction == \"mean\":\n",
    "            return weighted_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return weighted_loss.sum()\n",
    "        else:\n",
    "            return weighted_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sum of Normalized Weights: 1.0000000000072113\n",
      "Sum of Original Weights: 12000000000.086536\n",
      "Ratio of Normalized to Original Weights: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Sum of normalized weights vs. original weights\n",
    "sum_normalized = normalized_class_weights.values.sum()\n",
    "sum_original = class_weights.values.sum()\n",
    "\n",
    "print(f\"\\nSum of Normalized Weights: {sum_normalized}\")\n",
    "print(f\"Sum of Original Weights: {sum_original}\")\n",
    "\n",
    "# Ratio between normalized and original\n",
    "weight_ratio = sum_normalized / sum_original\n",
    "print(f\"Ratio of Normalized to Original Weights: {weight_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected Class Label Columns: ['class_id', 'x_min', 'y_min', 'x_max', 'y_max']\n",
      "\n",
      "Computed Class Weights:\n",
      " class_id    1.000000\n",
      "x_min       0.037184\n",
      "y_min       0.038043\n",
      "x_max       0.025615\n",
      "y_max       0.027742\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Identify only numeric class labels (exclude non-numeric columns like 'class_name')\n",
    "class_label_columns = train_labels_raw_df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Ensure we're not keeping unwanted columns like IDs\n",
    "exclude_columns = ['class_name', 'rad_id']  # Add more if needed\n",
    "class_label_columns = [col for col in class_label_columns if col not in exclude_columns]\n",
    "\n",
    "# Use only relevant class labels\n",
    "train_labels_cleaned = train_labels_raw_df[class_label_columns]\n",
    "\n",
    "# Debug: Print the selected columns\n",
    "print(\"Selected Class Label Columns:\", class_label_columns)\n",
    "\n",
    "# Compute positive sample counts per class\n",
    "positive_counts = train_labels_cleaned.sum(axis=0)\n",
    "\n",
    "# Avoid division by zero\n",
    "positive_counts = positive_counts.replace(0, 1e-6)\n",
    "\n",
    "# Compute class weights\n",
    "class_weights = len(train_labels_cleaned) / positive_counts\n",
    "\n",
    "# Normalize class weights\n",
    "normalized_class_weights = class_weights / class_weights.max()\n",
    "\n",
    "# Print computed class weights\n",
    "print(\"\\nComputed Class Weights:\\n\", normalized_class_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_name    NaN\n",
       "class_id       14\n",
       "rad_id        NaN\n",
       "x_min         NaN\n",
       "y_min         NaN\n",
       "x_max         NaN\n",
       "y_max         NaN\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_labels_raw_df.iloc[0, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Overall Sample Distribution:\n",
      "        Class      Count\n",
      "5       x_max  5413559.0\n",
      "6       y_max  4998487.0\n",
      "3       x_min  3729301.0\n",
      "4       y_min  3645029.0\n",
      "1    class_id     138669\n",
      "0  class_name          0\n",
      "2      rad_id          0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class_counts = train_labels_raw_df.iloc[:, 1:].sum()\n",
    "\n",
    "class_dist_df = pd.DataFrame({'Class': class_counts.index, 'Count': class_counts.values})\n",
    "class_dist_df = class_dist_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "print(\"\\nOverall Sample Distribution:\")\n",
    "print(class_dist_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(x='Class', y='Count', data=class_dist_df, palette=\"viridis\", hue='Class')\n",
    "plt.title(\"Overall Sample Distribution of Classes\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Class\", fontsize=12)\n",
    "plt.ylabel(\"Number of Positive Samples\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedWeightedFocalLoss(nn.Module):\n",
    "    def __init__(self, class_weights=None, gamma=2.0, alpha=0.25, reduction=\"mean\"):\n",
    "        super(ModifiedWeightedFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        \n",
    "        if class_weights is not None:\n",
    "            self.class_weights = class_weights\n",
    "        else:\n",
    "            self.class_weights = torch.ones(14)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "\n",
    "        probas = torch.sigmoid(inputs)\n",
    "\n",
    "        bce_loss = F.binary_cross_entropy(probas, targets, reduction=\"none\")\n",
    "        \n",
    "        # modulating factor (1 - p_t) ** gamma\n",
    "        p_t = probas * targets + (1 - probas) * (1 - targets)\n",
    "        modulating_factor = (1 - p_t) ** self.gamma\n",
    "        \n",
    "        # focal loss term\n",
    "        focal_loss = self.alpha * modulating_factor * bce_loss\n",
    "\n",
    "        weighted_loss = focal_loss * self.class_weights\n",
    "\n",
    "        # final loss\n",
    "        if self.reduction == \"mean\":\n",
    "            return weighted_loss.mean()\n",
    "        elif self.reduction == \"sum\":\n",
    "            return weighted_loss.sum()\n",
    "        else:\n",
    "            return weighted_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "class_name            0\n",
       "class_id         138669\n",
       "rad_id                0\n",
       "x_min         3729301.0\n",
       "y_min         3645029.0\n",
       "x_max         5413559.0\n",
       "y_max         4998487.0\n",
       "dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computed Class Weights:\n",
      " class_name    1.000000e+00\n",
      "class_id      7.211417e-12\n",
      "rad_id        1.000000e+00\n",
      "dtype: float64\n",
      "Sum of Normalized Weights: 2.000000000007211\n",
      "Sum of Original Weights: 24000000000.086536\n",
      "Ratio of Normalized to Original Weights: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Ensure all label columns are numeric\n",
    "train_labels_raw_df.iloc[:, 1:] = train_labels_raw_df.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Exclude non-class columns (metadata)\n",
    "exclude_columns = ['image_id', 'x_min', 'y_min', 'x_max', 'y_max']  \n",
    "class_label_columns = [col for col in train_labels_raw_df.columns if col not in exclude_columns]\n",
    "train_labels_cleaned = train_labels_raw_df[class_label_columns]\n",
    "\n",
    "# Convert all values to numeric and fill NaNs with 0\n",
    "train_labels_cleaned = train_labels_cleaned.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "# Compute positive sample counts per class\n",
    "total_samples = len(train_labels_cleaned)\n",
    "positive_counts = train_labels_cleaned.sum(axis=0)\n",
    "\n",
    "# Ensure no zero values in denominators to avoid division errors\n",
    "positive_counts = positive_counts.replace(0, 1e-6)\n",
    "\n",
    "# Compute and normalize class weights\n",
    "class_weights = total_samples / positive_counts\n",
    "normalized_class_weights = class_weights / class_weights.max()\n",
    "\n",
    "# Debugging: Print computed class weights\n",
    "print(\"\\nComputed Class Weights:\\n\", normalized_class_weights)\n",
    "print(f\"Sum of Normalized Weights: {normalized_class_weights.sum()}\")\n",
    "print(f\"Sum of Original Weights: {class_weights.sum()}\")\n",
    "print(f\"Ratio of Normalized to Original Weights: {normalized_class_weights.sum() / class_weights.sum():.4f}\")\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "weights = torch.tensor(normalized_class_weights.values, dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for visualization\n",
    "class_counts = train_labels_cleaned.sum()\n",
    "class_dist_df = pd.DataFrame({'Class': class_counts.index, 'Count': class_counts.values})\n",
    "class_dist_df = class_dist_df.sort_values(by='Count', ascending=False)\n",
    "\n",
    "# Print and visualize class distribution\n",
    "print(\"\\nOverall Sample Distribution:\")\n",
    "print(class_dist_df)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.barplot(x='Class', y='Count', data=class_dist_df, palette=\"viridis\")\n",
    "plt.title(\"Overall Sample Distribution of Classes\", fontsize=14, fontweight='bold')\n",
    "plt.xlabel(\"Class\", fontsize=12)\n",
    "plt.ylabel(\"Number of Positive Samples\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedWeightedFocalLoss(nn.Module):\n",
    "    def __init__(self, class_weights=None, gamma=2.0, alpha=0.25, reduction=\"mean\"):\n",
    "        super(ModifiedWeightedFocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        self.class_weights = class_weights if class_weights is not None else torch.ones(len(class_weights))\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        probas = torch.sigmoid(inputs)\n",
    "        bce_loss = F.binary_cross_entropy(probas, targets, reduction=\"none\")\n",
    "\n",
    "        # Compute modulating factor (1 - p_t)^gamma\n",
    "        p_t = probas * targets + (1 - probas) * (1 - targets)\n",
    "        modulating_factor = (1 - p_t) ** self.gamma\n",
    "\n",
    "        # Compute final focal loss\n",
    "        focal_loss = self.alpha * modulating_factor * bce_loss\n",
    "        weighted_loss = focal_loss * self.class_weights.to(inputs.device)\n",
    "\n",
    "        return weighted_loss.mean() if self.reduction == \"mean\" else weighted_loss.sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from PIL import Image\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define optimizer and scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=1e-4, steps_per_epoch=len(train_loader), epochs=50)\n",
    "\n",
    "# Define loss function with computed weights\n",
    "criterion = ModifiedWeightedFocalLoss(class_weights=weights, gamma=2.0, alpha=0.25, reduction=\"mean\")\n",
    "\n",
    "# Sample image processing\n",
    "image_path = r\"C:\\Users\\Documents\\cdac\\Extracted_DATASET\\train\\00000008_001.png\"\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "processed_image = swin_image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "# Extract model components\n",
    "pixel_values = processed_image[\"pixel_values\"]\n",
    "model_feature_extractor = model.feature_extractor\n",
    "model_classifier = model.classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'class_name', 'class_id', 'rad_id', 'x_min', 'y_min',\n",
      "       'x_max', 'y_max'],\n",
      "      dtype='object')\n",
      "[]\n",
      "                           image_id  class_name  class_id rad_id  x_min  \\\n",
      "0  4d4bffa2926bfcb4851b5e968136b047  No finding        14    R12    0.0   \n",
      "1  252353e9dd191831175accddf57e9625  No finding        14     R9    0.0   \n",
      "2  328e89c3bad7c078d2295a666e496bc7  No finding        14    R16    0.0   \n",
      "3  139282fc433f147aba91c6a99b8376c3  No finding        14     R8    0.0   \n",
      "4  d558c4e784e7c5808e1226f1f0f3559b  No finding        14     R1    0.0   \n",
      "\n",
      "   y_min  x_max  y_max  \n",
      "0    0.0    0.0    0.0  \n",
      "1    0.0    0.0    0.0  \n",
      "2    0.0    0.0    0.0  \n",
      "3    0.0    0.0    0.0  \n",
      "4    0.0    0.0    0.0  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12000 entries, 0 to 11999\n",
      "Data columns (total 8 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   image_id    12000 non-null  object \n",
      " 1   class_name  12000 non-null  object \n",
      " 2   class_id    12000 non-null  int64  \n",
      " 3   rad_id      12000 non-null  object \n",
      " 4   x_min       12000 non-null  float64\n",
      " 5   y_min       12000 non-null  float64\n",
      " 6   x_max       12000 non-null  float64\n",
      " 7   y_max       12000 non-null  float64\n",
      "dtypes: float64(4), int64(1), object(3)\n",
      "memory usage: 750.1+ KB\n",
      "None\n",
      "image_id      0\n",
      "class_name    0\n",
      "class_id      0\n",
      "rad_id        0\n",
      "x_min         0\n",
      "y_min         0\n",
      "x_max         0\n",
      "y_max         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_labels_df.columns)\n",
    "print([col for col in train_labels_df.columns if \"atelectasis\" in col.lower()])\n",
    "print(train_labels_df.head())\n",
    "print(train_labels_df.info())\n",
    "train_labels_df.rename(columns=lambda x: x.strip(), inplace=True)\n",
    "print(train_labels_df.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels_df[\"Atelectasis\"] = train_labels_df[\"class_name_Atelectasis\"]\n",
    "atelectasis_col = [col for col in train_labels_df.columns if \"Atelectasis\" in col]\n",
    "if atelectasis_col:\n",
    "    train_labels_df[\"Atelectasis\"] = train_labels_df[atelectasis_col[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['image_id', 'class_id', 'rad_id', 'x_min', 'y_min', 'x_max', 'y_max',\n",
      "       'class_name_Aortic enlargement', 'class_name_Atelectasis',\n",
      "       'class_name_Calcification', 'class_name_Cardiomegaly',\n",
      "       'class_name_Consolidation', 'class_name_ILD', 'class_name_Infiltration',\n",
      "       'class_name_Lung Opacity', 'class_name_No finding',\n",
      "       'class_name_Nodule/Mass', 'class_name_Other lesion',\n",
      "       'class_name_Pleural effusion', 'class_name_Pleural thickening',\n",
      "       'class_name_Pneumothorax', 'class_name_Pulmonary fibrosis',\n",
      "       'Atelectasis'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(train_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle), DataLoader(val_subset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_labels_df\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m---> 11\u001b[0m atelectasis_col \u001b[38;5;241m=\u001b[39m [col \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mtrain_labels_df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAtelectasis\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m col]\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m atelectasis_col:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAtelectasis\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column found in train_labels_df. Check column names.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "def get_data_loaders(self, fold):\n",
    "    atelectasis_col = [col for col in self.train_labels_df.columns if \"Atelectasis\" in col]\n",
    "    if not atelectasis_col:\n",
    "        raise ValueError(\"No 'Atelectasis' column found in train_labels_df. Check column names.\")\n",
    "\n",
    "    train_idx, val_idx = list(self.kfold.split(self.train_labels_df, self.train_labels_df[atelectasis_col[0]]))[fold]\n",
    "    train_subset = Subset(self.train_dataset, train_idx)\n",
    "    val_subset = Subset(self.train_dataset, val_idx)\n",
    "    return DataLoader(train_subset, batch_size=self.batch_size, shuffle=self.shuffle), DataLoader(val_subset, batch_size=self.batch_size, shuffle=False)\n",
    "print(train_labels_df.columns)\n",
    "atelectasis_col = [col for col in self.train_labels_df.columns if \"Atelectasis\" in col]\n",
    "if not atelectasis_col:\n",
    "    raise ValueError(\"No 'Atelectasis' column found in train_labels_df. Check column names.\")\n",
    "train_idx, val_idx = list(self.kfold.split(self.train_labels_df, self.train_labels_df[atelectasis_col[0]]))[fold]\n",
    "print(train_labels_df.head())  # Inspect first few rows\n",
    "print(train_labels_df.columns) # Check column names\n",
    "train_labels_df = pd.get_dummies(train_labels_df, columns=[\"class_name\"])\n",
    "train_labels_df[\"Atelectasis\"] = train_labels_df[\"class_name_Atelectasis\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedKFoldCV:\n",
    "    def __init__(self, train_dataset, train_labels_df, val_dataset=None, val_labels_df=None,\n",
    "                 k_folds=5, batch_size=32, shuffle=True, random_seed=42, log_dir=\"runs/stratified_kfold\"):\n",
    "        self.train_dataset = train_dataset\n",
    "        self.train_labels_df = train_labels_df\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_labels_df = val_labels_df\n",
    "        self.k_folds = k_folds\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.random_seed = random_seed\n",
    "        self.log_dir = log_dir\n",
    "        self.kfold = StratifiedKFold(n_splits=k_folds, shuffle=shuffle, random_state=random_seed)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        self.save_dir = \"model_weights\"\n",
    "\n",
    "    def get_data_loaders(self, fold):\n",
    "        train_idx, val_idx = list(self.kfold.split(self.train_labels_df, self.train_labels_df['Atelectasis']))[fold]\n",
    "        train_subset = Subset(self.train_dataset, train_idx)\n",
    "        val_subset = Subset(self.train_dataset, val_idx)\n",
    "        train_loader = DataLoader(train_subset, batch_size=self.batch_size, shuffle=self.shuffle)\n",
    "        val_loader = DataLoader(val_subset, batch_size=self.batch_size, shuffle=False)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train(self, model, optimizer, scheduler, criterion, device, train_loader, epoch):\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_samples = 0\n",
    "        all_train_labels = []\n",
    "        all_train_preds = []\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            running_train_loss += loss.item()\n",
    "            predictions = (outputs > 0.5).float()\n",
    "            correct_train_predictions += (predictions == labels).sum().item()\n",
    "            total_train_samples += labels.numel()\n",
    "            all_train_labels.append(labels.cpu().numpy())\n",
    "            all_train_preds.append(predictions.cpu().numpy())\n",
    "\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_predictions / total_train_samples * 100\n",
    "        all_train_labels = np.concatenate(all_train_labels)\n",
    "        all_train_preds = np.concatenate(all_train_preds)\n",
    "        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n",
    "        return avg_train_loss, train_accuracy, train_f1\n",
    "\n",
    "    def validate(self, model, criterion, device, val_loader, epoch):\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val_predictions = 0\n",
    "        total_val_samples = 0\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                running_val_loss += loss.item()\n",
    "                predictions = (outputs > 0.5).float()\n",
    "                correct_val_predictions += (predictions == labels).sum().item()\n",
    "                total_val_samples += labels.numel()\n",
    "                all_val_labels.append(labels.cpu().numpy())\n",
    "                all_val_preds.append(predictions.cpu().numpy())\n",
    "\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_predictions / total_val_samples * 100\n",
    "        all_val_labels = np.concatenate(all_val_labels)\n",
    "        all_val_preds = np.concatenate(all_val_preds)\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "        return avg_val_loss, val_accuracy, val_f1\n",
    "\n",
    "    def run(self, model, optimizer, scheduler, criterion, device, num_epochs=25):\n",
    "        fold_train_losses = []\n",
    "        fold_val_losses = []\n",
    "        fold_val_accuracies = []\n",
    "        fold_val_f1_scores = []\n",
    "\n",
    "        for fold in range(self.k_folds):\n",
    "            train_loader, val_loader = self.get_data_loaders(fold)\n",
    "            for epoch in range(num_epochs):\n",
    "                avg_train_loss, train_accuracy, train_f1 = self.train(\n",
    "                    model, optimizer, scheduler, criterion, device, train_loader, epoch\n",
    "                )\n",
    "                avg_val_loss, val_accuracy, val_f1 = self.validate(\n",
    "                    model, criterion, device, val_loader, epoch\n",
    "                )\n",
    "                self.writer.add_scalars('Loss', {'train': avg_train_loss, 'val': avg_val_loss}, epoch)\n",
    "                self.writer.add_scalars('Accuracy', {'train': train_accuracy, 'val': val_accuracy}, epoch)\n",
    "                self.writer.add_scalars('F1 Score', {'train': train_f1, 'val': val_f1}, epoch)\n",
    "\n",
    "            fold_train_losses.append(avg_train_loss)\n",
    "            fold_val_losses.append(avg_val_loss)\n",
    "            fold_val_accuracies.append(val_accuracy)\n",
    "            fold_val_f1_scores.append(val_f1)\n",
    "            torch.save(model.state_dict(), self.save_dir + f\"_fold{fold}\")\n",
    "\n",
    "        avg_train_loss = sum(fold_train_losses) / len(fold_train_losses)\n",
    "        avg_val_loss = sum(fold_val_losses) / len(fold_val_losses)\n",
    "        avg_val_accuracy = sum(fold_val_accuracies) / len(fold_val_accuracies)\n",
    "        avg_val_f1 = sum(fold_val_f1_scores) / len(fold_val_f1_scores)\n",
    "        return avg_train_loss, avg_val_loss, avg_val_accuracy, avg_val_f1\n",
    "\n",
    "stratified_kfold = StratifiedKFoldCV(\n",
    "    train_dataset=train_dataset,\n",
    "    train_labels_df=train_labels_raw_df,\n",
    "    val_dataset=val_dataset,\n",
    "    val_labels_df=val_labels_raw_df,\n",
    "    k_folds=5\n",
    ")\n",
    "\n",
    "avg_train_loss, avg_val_loss, avg_val_accuracy = stratified_kfold.run(model, optimizer, scheduler, criterion, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samples = len(train_labels_raw_df)\n",
    "\n",
    "positive_counts = train_labels_raw_df.iloc[:, 1:].sum(axis=0)\n",
    "\n",
    "class_weights = total_samples / positive_counts\n",
    "\n",
    "normalized_class_weights = class_weights / class_weights.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.03718364, 0.03804332, 0.02561513, 0.02774219])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_class_weights.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.8668919412990093"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_class_weights.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859.2573082865788"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights.values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(normalized_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = torch.tensor(normalized_class_weights.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0217, 0.0829, 0.0187, 0.0123, 0.0481, 0.0396, 0.1565, 0.0482, 0.0519,\n",
       "        0.0991, 0.0838, 0.1314, 0.0727, 1.0000], dtype=torch.float64)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "scheduler = OneCycleLR(optimizer, max_lr=1e-4, steps_per_epoch=len(train_loader), epochs=50)\n",
    "criterion = ModifiedWeightedFocalLoss(class_weights=weights, gamma=2.0, alpha=0.25, reduction=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sample_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = r\"C:\\Users\\Documents\\cdac\\Extracted_DATASET\\train\\00000008_001.png\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "processed_image = swin_image_processor(image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pixel_values'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_image.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 224, 224])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_image[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_image[\"pixel_values\"].squeeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=1000, bias=True)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin_model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelClassifier(\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "    (1): GELU(approximate='none')\n",
       "    (2): Dropout(p=0.3, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=14, bias=True)\n",
       "    (4): Sigmoid()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = processed_image[\"pixel_values\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature_extractor = model.feature_extractor\n",
    "model_classifier = model.classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StratifiedKFoldCV:\n",
    "    def __init__(self, train_dataset, train_labels_df, val_dataset=None, val_labels_df=None, \n",
    "                 k_folds=5, batch_size=32, shuffle=True, random_seed=42, log_dir=\"runs/stratified_kfold\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_dataset (Dataset): The training dataset (e.g., MedDataset) to be split into K folds.\n",
    "            train_labels_df (DataFrame): The labels dataframe for the training dataset.\n",
    "            val_dataset (Dataset, optional): The validation dataset for evaluating the model on fixed data.\n",
    "            val_labels_df (DataFrame, optional): The labels dataframe for the validation dataset.\n",
    "            k_folds (int): The number of folds for cross-validation.\n",
    "            batch_size (int): Batch size for the DataLoader.\n",
    "            shuffle (bool): Whether to shuffle the dataset before splitting.\n",
    "            random_seed (int): Random seed for reproducibility.\n",
    "            log_dir (str): Directory to store TensorBoard logs.\n",
    "        \"\"\"\n",
    "        self.train_dataset = train_dataset\n",
    "        self.train_labels_df = train_labels_df\n",
    "        self.val_dataset = val_dataset\n",
    "        self.val_labels_df = val_labels_df\n",
    "        self.k_folds = k_folds\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.random_seed = random_seed\n",
    "        self.log_dir = log_dir\n",
    "        self.kfold = StratifiedKFold(n_splits=k_folds, shuffle=shuffle, random_state=random_seed)\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "        \n",
    "        self.save_dir = \"model_weights\"\n",
    "        \n",
    "\n",
    "    def get_data_loaders(self, fold):\n",
    "        \"\"\"\n",
    "        Get the train and validation DataLoaders for the current fold.\n",
    "\n",
    "        Args:\n",
    "            fold (int): The current fold number.\n",
    "\n",
    "        Returns:\n",
    "            train_loader (DataLoader): DataLoader for the training set of the current fold.\n",
    "            val_loader (DataLoader): DataLoader for the validation set of the current fold.\n",
    "        \"\"\"\n",
    "        # Get the indices for train and validation splits for the current fold\n",
    "        train_idx, val_idx = list(self.kfold.split(self.train_labels_df, self.train_labels_df['Atelectasis']))[fold]\n",
    "\n",
    "        # Create Subsets for training and validation sets\n",
    "        train_subset = Subset(self.train_dataset, train_idx)\n",
    "        val_subset = Subset(self.train_dataset, val_idx)\n",
    "\n",
    "        # Create DataLoaders\n",
    "        train_loader = DataLoader(train_subset, batch_size=self.batch_size, shuffle=self.shuffle)\n",
    "        val_loader = DataLoader(val_subset, batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    def train(self, model, optimizer, scheduler, criterion, device, train_loader, epoch):\n",
    "        \"\"\"\n",
    "        Train the model for one epoch.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The model to train.\n",
    "            optimizer (torch.optim.Optimizer): The optimizer.\n",
    "            scheduler (torch.optim.lr_scheduler._LRScheduler): The learning rate scheduler.\n",
    "            criterion (nn.Module): The loss function.\n",
    "            device (torch.device): The device (GPU or CPU).\n",
    "            train_loader (DataLoader): DataLoader for the training set.\n",
    "\n",
    "        Returns:\n",
    "            avg_train_loss (float): Average training loss for the epoch.\n",
    "            train_accuracy (float): Training accuracy for the epoch.\n",
    "            train_f1 (float): Training F1 score for the epoch.\n",
    "        \"\"\"\n",
    "        model.train()\n",
    "        running_train_loss = 0.0\n",
    "        correct_train_predictions = 0\n",
    "        total_train_samples = 0\n",
    "        all_train_labels = []\n",
    "        all_train_preds = []\n",
    "\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Training Epoch {epoch + 1}\", leave=False):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()  # Zero out gradients from the previous step\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping (to avoid exploding gradients)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "\n",
    "            # Optimizer step\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update learning rate scheduler\n",
    "            scheduler.step()\n",
    "\n",
    "            # Accumulate loss for logging\n",
    "            running_train_loss += loss.item()\n",
    "\n",
    "            # For multi-label classification, use thresholding at 0.5 to predict class labels\n",
    "            predictions = (outputs > 0.5).float()  # Sigmoid thresholding\n",
    "            correct_train_predictions += (predictions == labels).sum().item()\n",
    "            total_train_samples += labels.numel()\n",
    "\n",
    "            # Collect all labels and predictions for F1 score\n",
    "            all_train_labels.append(labels.cpu().numpy())\n",
    "            all_train_preds.append(predictions.cpu().numpy())\n",
    "\n",
    "        # Average training loss and accuracy\n",
    "        avg_train_loss = running_train_loss / len(train_loader)\n",
    "        train_accuracy = correct_train_predictions / total_train_samples * 100  # In percentage\n",
    "\n",
    "        # Flatten lists for F1 calculation\n",
    "        all_train_labels = np.concatenate(all_train_labels)\n",
    "        all_train_preds = np.concatenate(all_train_preds)\n",
    "\n",
    "        # F1 score\n",
    "        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%, Train F1: {train_f1:.4f}\")\n",
    "\n",
    "        return avg_train_loss, train_accuracy, train_f1\n",
    "\n",
    "    def validate(self, model, criterion, device, val_loader, epoch):\n",
    "        \"\"\"\n",
    "        Validate the model on the validation set.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The model to validate.\n",
    "            criterion (nn.Module): The loss function.\n",
    "            device (torch.device): The device (GPU or CPU).\n",
    "            val_loader (DataLoader): DataLoader for the validation set.\n",
    "\n",
    "        Returns:\n",
    "            avg_val_loss (float): Average validation loss for the epoch.\n",
    "            val_accuracy (float): Validation accuracy for the epoch.\n",
    "            val_f1 (float): Validation F1 score for the epoch.\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        running_val_loss = 0.0\n",
    "        correct_val_predictions = 0\n",
    "        total_val_samples = 0\n",
    "        all_val_labels = []\n",
    "        all_val_preds = []\n",
    "\n",
    "        with torch.no_grad():  # Disable gradient calculation for validation\n",
    "            for images, labels in tqdm(val_loader, desc=\"Validating\", leave=False):\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "                # Forward pass\n",
    "                outputs = model(images)\n",
    "\n",
    "                # Calculate loss\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                # Accumulate validation loss\n",
    "                running_val_loss += loss.item()\n",
    "\n",
    "                # For multi-label classification, use thresholding at 0.5 to predict class labels\n",
    "                predictions = (outputs > 0.5).float()  # Sigmoid thresholding\n",
    "                correct_val_predictions += (predictions == labels).sum().item()\n",
    "                total_val_samples += labels.numel()\n",
    "\n",
    "                # Collect all labels and predictions for F1 score\n",
    "                all_val_labels.append(labels.cpu().numpy())\n",
    "                all_val_preds.append(predictions.cpu().numpy())\n",
    "\n",
    "        # Average validation loss and accuracy\n",
    "        avg_val_loss = running_val_loss / len(val_loader)\n",
    "        val_accuracy = correct_val_predictions / total_val_samples * 100  # In percentage\n",
    "\n",
    "        # Flatten lists for F1 calculation\n",
    "        all_val_labels = np.concatenate(all_val_labels)\n",
    "        all_val_preds = np.concatenate(all_val_preds)\n",
    "\n",
    "        # F1 score\n",
    "        val_f1 = f1_score(all_val_labels, all_val_preds, average='macro')\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1} - Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%, Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        return avg_val_loss, val_accuracy, val_f1\n",
    "\n",
    "    def plot_losses(self, train_losses, val_losses, epoch):\n",
    "        \"\"\"\n",
    "        Plot the training and validation loss for the current epoch.\n",
    "\n",
    "        Args:\n",
    "            train_losses (list): List of training losses.\n",
    "            val_losses (list): List of validation losses.\n",
    "            epoch (int): Current epoch number.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label=\"Train Loss\", color=\"blue\")\n",
    "        plt.plot(val_losses, label=\"Val Loss\", color=\"red\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(f\"Losses for Epoch {epoch}\")\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"epoch_{epoch}_loss_plot.png\")\n",
    "        plt.close()\n",
    "\n",
    "    def run(self, model, optimizer, scheduler, criterion, device, num_epochs=25):\n",
    "        \"\"\"\n",
    "        Run Stratified K-Fold Cross Validation.\n",
    "\n",
    "        Args:\n",
    "            model (nn.Module): The model to train.\n",
    "            optimizer (torch.optim.Optimizer): The optimizer.\n",
    "            scheduler (torch.optim.lr_scheduler._LRScheduler): The learning rate scheduler.\n",
    "            criterion (nn.Module): The loss function.\n",
    "            device (torch.device): The device (GPU or CPU).\n",
    "            num_epochs (int): Number of epochs for training.\n",
    "\n",
    "        Returns:\n",
    "            avg_train_loss (float): Average training loss across all folds.\n",
    "            avg_val_loss (float): Average validation loss across all folds.\n",
    "            avg_val_accuracy (float): Average validation accuracy across all folds.\n",
    "        \"\"\"\n",
    "        fold_train_losses = []\n",
    "        fold_val_losses = []\n",
    "        fold_val_accuracies = []\n",
    "        fold_val_f1_scores = []\n",
    "\n",
    "        i = 0\n",
    "        \n",
    "        for fold in range(self.k_folds):\n",
    "            print(f\"Fold {fold + 1}/{self.k_folds}\")\n",
    "            train_loader, val_loader = self.get_data_loaders(fold)\n",
    "\n",
    "            # Initialize loss lists for plotting\n",
    "            train_losses = []\n",
    "            val_losses = []\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "\n",
    "                # Train the model for the current epoch\n",
    "                avg_train_loss, train_accuracy, train_f1 = self.train(\n",
    "                    model, optimizer, scheduler, criterion, device, train_loader, epoch\n",
    "                )\n",
    "\n",
    "                # Validate the model for the current epoch\n",
    "                avg_val_loss, val_accuracy, val_f1 = self.validate(\n",
    "                    model, criterion, device, val_loader, epoch\n",
    "                )\n",
    "\n",
    "                # Log metrics to TensorBoard\n",
    "                self.writer.add_scalars('Loss', {'train': avg_train_loss, 'val': avg_val_loss}, epoch)\n",
    "                self.writer.add_scalars('Accuracy', {'train': train_accuracy, 'val': val_accuracy}, epoch)\n",
    "                self.writer.add_scalars('F1 Score', {'train': train_f1, 'val': val_f1}, epoch)\n",
    "\n",
    "                # Save losses for plotting\n",
    "                train_losses.append(avg_train_loss)\n",
    "                val_losses.append(avg_val_loss)\n",
    "\n",
    "                # Plot losses every 10 epochs\n",
    "                self.plot_losses(train_losses, val_losses, epoch + 1)\n",
    "\n",
    "            fold_train_losses.append(avg_train_loss)\n",
    "            fold_val_losses.append(avg_val_loss)\n",
    "            fold_val_accuracies.append(val_accuracy)\n",
    "            fold_val_f1_scores.append(val_f1)\n",
    "\n",
    "            save_path = self.save_dir + f\"_fold{i}\"\n",
    "            i += 1\n",
    "            \n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            \n",
    "            print(f\"Fold {fold + 1} - Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.2f}%\")\n",
    "            print(f\"Fold {fold + 1} - Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}%\")\n",
    "            print(f\"Fold {fold + 1} - Val F1: {val_f1:.4f}\")\n",
    "\n",
    "        # Calculate average metrics across all folds\n",
    "        avg_train_loss = sum(fold_train_losses) / len(fold_train_losses)\n",
    "        avg_val_loss = sum(fold_val_losses) / len(fold_val_losses)\n",
    "        avg_val_accuracy = sum(fold_val_accuracies) / len(fold_val_accuracies)\n",
    "        avg_val_f1 = sum(fold_val_f1_scores) / len(fold_val_f1_scores)\n",
    "\n",
    "        print(f\"Average Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"Average Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Average Val Accuracy: {avg_val_accuracy:.2f}%\")\n",
    "        print(f\"Average Val F1: {avg_val_f1:.4f}\")\n",
    "\n",
    "        return avg_train_loss, avg_val_loss, avg_val_accuracy, avg_val_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "stratified_kfold = StratifiedKFoldCV(\n",
    "    train_dataset=train_dataset, \n",
    "    train_labels_df=train_labels_raw_df,\n",
    "    val_dataset=val_dataset,\n",
    "    val_labels_df=val_labels_raw_df,\n",
    "    k_folds=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25fb4d942a0c406baa5bb838ebcf92d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 1:   0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.0073, Train Accuracy: 88.60%, Train F1: 0.0526\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b3f2cc770d4df092b6ae3e04b88478",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Val Loss: 0.0059, Val Accuracy: 94.83%, Val F1: 0.0000\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ab333f7841478089b1f5129bc1a53e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 2:   0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Train Loss: 0.0058, Train Accuracy: 94.90%, Train F1: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f0c689f700a4b55a8579075b038f7a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - Val Loss: 0.0058, Val Accuracy: 94.83%, Val F1: 0.0000\n",
      "Epoch 3/25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398ca35ac2744fc4a7f4ba5587dc32c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 3:   0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Train Loss: 0.0058, Train Accuracy: 94.90%, Train F1: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77a2a130b7034618ac1e9f36dc31f190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 - Val Loss: 0.0058, Val Accuracy: 94.83%, Val F1: 0.0000\n",
      "Epoch 4/25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715a792367304a859e9edf66802edde2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 4:   0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Train Loss: 0.0058, Train Accuracy: 94.90%, Train F1: 0.0000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a39561baeaf2475ca7471ef38b50b212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/133 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 - Val Loss: 0.0058, Val Accuracy: 94.83%, Val F1: 0.0000\n",
      "Epoch 5/25\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef8376d386ba4cd783e819298d2dba13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Epoch 5:   0%|          | 0/530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "avg_train_loss, avg_val_loss, avg_val_accuracy = stratified_kfold.run(model, optimizer, scheduler, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiLabelModel(\n",
       "  (feature_extractor): SwinModelWithPEFT(\n",
       "    (backbone): SwinForImageClassification(\n",
       "      (swin): SwinModel(\n",
       "        (embeddings): SwinEmbeddings(\n",
       "          (patch_embeddings): SwinPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): SwinEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-5): 6 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "      )\n",
       "      (classifier): Identity()\n",
       "    )\n",
       "  )\n",
       "  (classifier): MultiLabelClassifier(\n",
       "    (classifier): Sequential(\n",
       "      (0): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (1): GELU(approximate='none')\n",
       "      (2): Dropout(p=0.3, inplace=False)\n",
       "      (3): Linear(in_features=512, out_features=14, bias=True)\n",
       "      (4): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
